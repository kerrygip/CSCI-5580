{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Branded_Logo_CUDenver.PNG\" width=\"150\">\n",
    "\n",
    "## <center>CSCI 4580/5580 - Data Science – Spring 2022</center>\n",
    "<center>Assignment 3: KNN, Naive Bayes and Text Feature Selection</center><center><font color='red'>Deadline: May 6, 2022 - 11:59 PM</font></center><center>Total Points: 100</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "- Please note that this assignment must be done individually. By submitting this assignment, you certify that this is your own work, your code will be checked against other submissions and resources using automatic tools. Everyone should be getting a hands on experience in this course. You are free to discuss course material with fellow students, and we encourage you to use Internet resources to aid your understanding, but the work you turn in, including all code and answers, must be your own work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverables\n",
    "You need to submit a single .ipynb file on Canvas, named your-lastname_your-first-name.ipynb. For example, if your name is John Smith, you should name the file smith_john.ipynb.\n",
    "- Please do not include extra files such as the input datasets in your submission.\n",
    "- Answer Questions 1 - 7 in the designated cells. Please do not add or remove any cells. \n",
    "- Please download your submission file after submission and make sure it is not corrupted. Use the 'Run All' option from the 'Cell' menu to ensure all cells run without any issues. We will not be responsible for corrupted submissions and will not take a resubmission after the deadline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need Help?\n",
    "If you need help with this lab, please email me at sundous.hussein@ucdenver.edu or come to my office hours. We also encourage you to ask your questions on the designated channel for the assignment on Microsoft Teams. This way, you may receive assistance from your classmates that might’ve ran through the same issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's Get Started!\n",
    "===\n",
    "## Overview\n",
    "\n",
    "Supervised Learning is the process of learning based on known examples which have both an input and a labeled output. Using the data and labels provided in the training dataset, models can learn a prediction function or mapping, which may then be applied to unseen test data. As we discussed in the class, some common methods for supervised learning include the K-Nearest-Neighbors Classifier (KNN) and Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning on Text Data\n",
    "\n",
    "For this assignment, we'll explore kNN and Naive Bayes further and look at different feature selection strategies. First of all, download the dataset for this assignment from Canvas and unpack into an \"Assignment 3\" directory. \n",
    "\n",
    "The dataset is from the RCV1 v2 corpus which is a collection of news articles and category labels. There are 103 categories, and each document can lie in one or more categories. \n",
    "\n",
    "The sparse train and test matrices are encoded as nnz x 3 integer matrices, where nnz is the total number of words in all docs. Each row of these matrices contains (doc, word, count) triples. We will store those in sparse matrices whose dimensions are ndocs x nwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application\n",
    "\n",
    "Your assignment is to perform supervised learning using both the KNN and Naive Bayes classifiers, and evaluate the results. You will also perform multiple types of text feature selection to choose subsets of the total features to use for classification, and evaluate these results as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now read the data files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iwords = np.loadtxt(\"Dataset/data/words.imat.txt\",dtype=np.int32,)          # training data matrix in nnz x 3 form - rows are (doc, word, count) triples\n",
    "cats = np.loadtxt(\"Dataset/data/cats.imat.txt\",dtype=np.int32)             # training labels in an ndocs x ncats matrix\n",
    "tiwords = np.loadtxt(\"Dataset/data/testwords.imat.txt\",dtype=np.int32)     # test data matrix in nnz x 3 form\n",
    "tcats = np.loadtxt(\"Dataset/data/testcats.imat.txt\",dtype=np.int32)        # testing labels in an ndocs x ncats matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iwords and tiwords encode the sparse train and test matrices. Each row of these matrices contains (doc, word, count) triples. We will store those in sparse matrices whose dimensions are ndocs x nwords. The exact matrix type is CSR which stands for compressed sparse rows. Look up its description here: https://en.wikipedia.org/wiki/Sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fwords = iwords[:,2]\n",
    "train = sp.csr_matrix((fwords, (iwords[:,1], iwords[:,0])))\n",
    "ndocs = train.shape[0]\n",
    "nwords = train.shape[1]\n",
    "\n",
    "test = sp.csr_matrix((tiwords[:,2], (tiwords[:,1], tiwords[:,0])),shape=(4000,nwords))  # need to match the number of cols (words)\n",
    "ntdocs = test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the dataset has 103 category labels, we'll focus on training a single category. We will use Category 6, which is fairly evenly balanced between positives and negatives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat6 = cats[:,6]\n",
    "tcat6 = tcats[:,6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: KNN Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we'll train Scikit-Learn's builtin kNN classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knnc = KNeighborsClassifier(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then \"fit\" it to the data. Notice how fast this is! Of course, training kNN is basically a no-op, and predicting is where all the work lies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knnc.fit(train, cat6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try predicting. This will actually take some time, but hopefully only a few tens of seconds..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = knnc.predict(test);preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compute the accuracy on the actual test set labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82225"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(tcat6 == preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad, but kNN is very sensitive to the distance function as we discussed in class. Let's try first normalizing each row of \"train\" and \"test\" by the L2-norm of that row, i.e. we will divide by the square root of the sum of the squared word counts in the row. Comparing the normalized documents should do much better. \n",
    "\n",
    "Remember, the L2-Norm for a vector $x$ can be written as:\n",
    "\n",
    "$$  \\lVert x \\rVert_2  = \\sqrt{x_1^2 + x_1^2 + \\cdots + x_n^2}$$\n",
    "\n",
    "## Q1 [20 points]: \n",
    "\n",
    "Create two new variables `trainnmat` and `testnmat` which contain the L2-normalized versions of `train` and `test` respectively.\n",
    "\n",
    "L2-normalization should be performed across each row $r$ of the `train` and `test` matrices, so that:\n",
    "\n",
    "$$ \\sqrt{r_1^2 + r_2^2 + \\cdots + r_n^2} = 1 $$\n",
    "\n",
    "However, you must compute the L2-normalized matrices using `numpy` and `scipy.sparse` operations, using the scikit-learn `normalize` function to generate your answer is not valid.\n",
    "Also ensure that your output matrix is still sparse. Many NumPy operations will either fail or convert CSR matrices to dense matrices. We prefer to keep this matrix in CSR format for the duration of this lab.\n",
    "\n",
    "The documentation for the following sparse matrix function may be useful for you:\n",
    "\n",
    "- `scipy.sparse.csr_matrix` [Docs](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html)\n",
    "- `scipy.sparse.csr_matrix.sum` [Docs](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.sum.html)\n",
    "- `scipy.sparse.csr_matrix.power` [Docs](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.power.html#scipy.sparse.csr_matrix.power)\n",
    "- `scipy.sparse.spdiags` [Docs](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.spdiags.html)\n",
    "\n",
    "Above we have already imported `scipy.sparse` as `sp`.\n",
    "\n",
    "**HINT:** For both `train` and `test` you need to structure your normalization as a matrix multiply operation. Start by creating a diagonal matrix where the squared sums of each row are along the diagonal elements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10000x31331 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 801667 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normalize lol \n",
    "trainnmat = preprocessing.normalize(train)\n",
    "testnmat = preprocessing.normalize(test)\n",
    "\n",
    "trainnmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST CASES - DO NOT MODIFY\n",
    "# These test cases check that your normalization is close (within numerical precision) of the answer\n",
    "# from sklearn's normalize function.\n",
    "from sklearn.preprocessing import normalize\n",
    "skl_norm_train = normalize(train)\n",
    "skl_norm_train.sort_indices()\n",
    "\n",
    "skl_norm_test = normalize(test)\n",
    "skl_norm_test.sort_indices()\n",
    "\n",
    "trainnmat.sort_indices()\n",
    "testnmat.sort_indices()\n",
    "\n",
    "assert(np.allclose(trainnmat.data,skl_norm_train.data))\n",
    "assert(np.allclose(testnmat.data,skl_norm_test.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train and evaluate a knn model using the normalized datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90275"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knnc.fit(trainnmat, cat6)\n",
    "preds = knnc.predict(testnmat);preds\n",
    "np.mean(tcat6 == preds)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2 [10 points]: \n",
    "What is the accuracy of your kNN classifier on the normalized data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add your answer to Question 2 here\n",
    "90.275%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Naive Bayes Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a Multinomial Naive Bayes classifier. The multinomial distribution models each word in a document as being drawn from a category-specific probability distribution over the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can fit the NB model, which simply means tabulating the word counts for each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.fit(train, cat6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and predict the category labels for cat6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 1, 0, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = mnb.predict(test);preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3 [10 points]: \n",
    "\n",
    "Let's check the accuracy below. How did NB do compared to kNN? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8975"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(tcat6 == preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add your answer to Question 3 here  \n",
    "About the same. 89% vs 90%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Frequency-Based Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We argued that frequency-based feature selection is useful approach to culling features to reduce the size of a model. In some cases, accuracy improves as well. \n",
    "\n",
    "Let's construct a matrix rcounts which contains the counts of features from the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[3728, 4438, 3488, ...,    1,    1,    1]], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rcounts = (train>0).sum(0)\n",
    "rcounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31331"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rcounts.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll find the indices of non-zero elements of the predicate (rcounts >= count). These are the features that occurred at least count times. ii is the indices of these features.\n",
    "\n",
    "We then extract the columns of the train and test matrices corresponding to those high-count features. The last line tells us how many features we kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0     1     2 ... 30215 30216 30227]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18330"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 2\n",
    "ii0 = np.asarray(np.nonzero(rcounts >= threshold)[1])\n",
    "print(ii0)\n",
    "ii = ii0.reshape(ii0.size,)\n",
    "\n",
    "strain = train[:,ii]\n",
    "stest = test[:,ii]\n",
    "ii.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we run training and evaluation on train/test data restricted to those columns to get an accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8955"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.fit(strain, cat6)\n",
    "preds = mnb.predict(stest);preds\n",
    "np.mean(tcat6 == preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4 [20 points]\n",
    "For the following question, use the `threshold` variable set above.\n",
    "\n",
    "For threshold counts of 1, 2, 5, 10, 20, tabulate the count, number of features kept, and the accuracy score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Threshold</th>\n",
       "      <th>Count</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Num Features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>31331</td>\n",
       "      <td>0.89750</td>\n",
       "      <td>31331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>31331</td>\n",
       "      <td>0.89550</td>\n",
       "      <td>18330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>31331</td>\n",
       "      <td>0.89000</td>\n",
       "      <td>9632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>31331</td>\n",
       "      <td>0.88675</td>\n",
       "      <td>6169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>31331</td>\n",
       "      <td>0.88400</td>\n",
       "      <td>4046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Threshold  Count  Accuracy  Num Features\n",
       "0          1  31331   0.89750         31331\n",
       "1          2  31331   0.89550         18330\n",
       "2          5  31331   0.89000          9632\n",
       "3         10  31331   0.88675          6169\n",
       "4         20  31331   0.88400          4046"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add your answer to Question 4 here\n",
    "threshold_list = [1,2,5,10,20]\n",
    "count = []\n",
    "accuracy_score = []\n",
    "features = []\n",
    "\n",
    "for i in threshold_list:\n",
    "    #counts\n",
    "    rcounts = (train>0).sum(0)\n",
    "    count.append(rcounts.size)\n",
    "    \n",
    "    #features\n",
    "    ii0 = np.asarray(np.nonzero(rcounts >= i)[1])\n",
    "    ii = ii0.reshape(ii0.size,)\n",
    "    strain = train[:,ii]\n",
    "    stest = test[:,ii]\n",
    "    features.append(ii.size)\n",
    "    \n",
    "    #accuracy \n",
    "    mnb.fit(strain, cat6)\n",
    "    preds = mnb.predict(stest);preds\n",
    "    accuracy_score.append(np.mean(tcat6 == preds))\n",
    "    \n",
    "    \n",
    "\n",
    "table = pd.DataFrame(list(zip(threshold_list, count, accuracy_score, features)), columns =[\"Threshold\",\"Count\", \"Accuracy\",\"Num Features\"])   \n",
    "table\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5 [10 points]\n",
    "\n",
    "How did frequency-based feature selection affect accuracy? How much was the model size (proportional to words kept) reduced? \n",
    "\n",
    "(`ii.size` is the reduced model size, `rcounts.size` is the original model size.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add your answer to Question 5 here  \n",
    "\n",
    "It slightly reduced the accuracy. The model sized was reduced to a fraction(12%) of what it used to be and just chose the most frequent terms. The higher the number of features, the better frequency based feature selection performs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Feature Selection Based on Statistical Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a more sophisticated method to select the (word) features. Since we have count data, the Chi-Squared test is appropriate. Remember that Chi-squared can be used on 2x2 contingency tables to determine if there is an \"interaction\". In this case, we test if the label interacts with the feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcounts = np.zeros((4,nwords))           # holds the counts of various combinations of label and feature\n",
    "expected = np.zeros((4,nwords))          # holds the predicted counts of those same combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3626. 3265. 4146. ... 5429. 5429. 5429.]\n"
     ]
    }
   ],
   "source": [
    "mcat6 = cat6.reshape(1,ndocs)\n",
    "fcounts[0,:] = mcat6 * (train > 0)                                   # label and word both = 1\n",
    "fcounts[1,:] = (1-mcat6) * (train > 0)                               # label = 0, word = 1\n",
    "fcounts[2,:] = np.sum(mcat6) * np.ones((1,nwords)) - fcounts[0,:]    # label = 1, word = 0\n",
    "fcounts[3,:] = np.sum(1-mcat6) * np.ones((1,nwords)) - fcounts[1,:]  # label = 0, word = 0\n",
    "print(fcounts[3,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the definition of the CHI-squared test here:\n",
    "https://en.wikipedia.org/wiki/Chi-squared_test ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "allf = np.sum(fcounts,axis=0)\n",
    "pword0 = (fcounts[2,:] + fcounts[3,:]) / allf    # prob word = 0\n",
    "pword1 = (fcounts[0,:] + fcounts[1,:]) / allf    # prob word = 1\n",
    "pcat0 = (fcounts[1,:] + fcounts[3,:]) / allf     # prob label = 0\n",
    "pcat1 = (fcounts[0,:] + fcounts[2,:]) / allf     # prob label = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If label and word occurence are independent, then the expected count should be the product of the total count and the two corresponding probabilities:\n",
    "\n",
    "\n",
    "\n",
    "## Q6 [20 points]: \n",
    "\n",
    "Implement the expected counts here. \n",
    "\n",
    "**HINT** This can be computed entirely with the variables from the previous code cell, with no other variables needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.703696e-01 2.028166e-01 1.594016e-01 ... 4.570000e-05 4.570000e-05\n",
      " 4.570000e-05]\n"
     ]
    }
   ],
   "source": [
    "#TODO: Implement this\n",
    "#product of the total count and the two corresponding probabilities\n",
    "expected[0,:] =   (pcat1-1) * (pword1-1) # expected count with label = 1, word = 1\n",
    "expected[1,:] =   (pcat0-1) * (pword1-1) # expected count with label = 0, word = 1\n",
    "expected[2,:] =   (pcat1-1) * (pword0-1) # expected count with label = 1, word = 0\n",
    "expected[3,:] =   (pcat0-1) * (pword0-1) # expected count with label = 0, word = 0\n",
    "print(expected[3,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the chi-squared statistic value, we want the sum of the squared differences between actual and predicted counts, normalized by the expected counts. The following two cells perform that calculation. We have to guard against divide-by-zero so we use a small lower-bound on the denominator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = fcounts - expected\n",
    "safeexpected = np.maximum(expected,1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31331,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chisquaredstat = np.sum(np.multiply(diff, diff) / safeexpected, axis=0)\n",
    "chisquaredstat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.33962370e+08, 1.09982501e+08, 1.56650454e+08, ...,\n",
       "       1.02956680e+12, 1.02956680e+12, 1.02956680e+12])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chisquaredstat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use the chi-squared statistic to choose the best features. The CHI-squared stastistic is a measure of interaction between a word feature and the label. The higher its value, the better that word should be as a predictor of the category. We set a threshold and take the words whose chi-squared statistic is above this value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31331,)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chisqthreshold = 7.879\n",
    "ii0 = np.asarray(np.nonzero(chisquaredstat > chisqthreshold)[0])\n",
    "ii = ii0.reshape(ii0.size,);ii.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we use the indices of good features selected by the chi-squared filter to train and evaluate the Naive Bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8975"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strain = train[:,ii]\n",
    "stest = test[:,ii]\n",
    "mnb.fit(strain, cat6)\n",
    "preds = mnb.predict(stest);preds\n",
    "np.mean(tcat6 == preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7 [10 points]\n",
    "\n",
    "Using a chi-sqaured critical value table (one can be found here: [Link](https://people.richland.edu/james/lecture/m170/tbl-chi.html)), find the table row for 1 degree of freedom (`df=1`), and set `chisqthreshold` to a few different values from the table. Record your accuracies here.\n",
    "\n",
    "In a few sentences describe how well this feature selection process works on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your answer to Question 7 here\n",
    "#0.001\t0.004\t0.016\t2.706\t3.841\t5.024\t6.635\t7.879"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, given that I've played around with the calculations for chisquared and probably have done it wrong. I've still gotten soemthing close to 90% that doesn't change regardless of the value that I used for degrees of freedom, I guess it works pretty well given that it produce a high percentage of accuracy LOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
