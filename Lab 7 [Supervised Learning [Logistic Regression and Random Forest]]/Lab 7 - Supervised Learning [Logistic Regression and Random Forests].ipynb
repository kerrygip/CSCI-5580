{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Branded_Logo_CUDenver.PNG\" width=\"150\">\n",
    "\n",
    "## <center>CSCI 4580/5580 - Data Science – Spring 2022</center>\n",
    "<center>Lab 7: Logistic Regression and Random Forests</center><center><font color='red'>Deadline: April 25, 2022 - 11:59 PM</font></center><center>Total Points: 100</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "- Please note that this lab must be done individually. By submitting this lab, you certify that this is your own work, your code will be checked against other submissions and resources using automatic tools. Everyone should be getting a hands on experience in this course. You are free to discuss course material with fellow students, and we encourage you to use Internet resources to aid your understanding, but the work you turn in, including all code and answers, must be your own work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverables\n",
    "You need to submit a single .ipynb file on Canvas, named your-lastname_your-first-name.ipynb. For example, if your name is John Smith, you should name the file smith_john.ipynb.\n",
    "- Please do not include extra files such as the input datasets in your submission.\n",
    "- Answer Questions 1 - 7 in the designated cells. Please do not add or remove any cells. \n",
    "- Please download your submission file after submission and make sure it is not corrupted. Use the 'Run All' option from the 'Cell' menu to ensure all cells run without any issues. We will not be responsible for corrupted submissions and will not take a resubmission after the deadline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need Help?\n",
    "If you need help with this lab, please email me at sundous.hussein@ucdenver.edu or come to my office hours. We also encourage you to ask your questions on the designated channel for the lab on Microsoft Teams. This way, you may receive assistance from your classmates that might’ve ran through the same issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given enough data, Logisitic Regression should always do at least as well as Naive Bayes on binary-feature data. Let's try that out on the text dataset (named <b>words</b>) that can be downloaded from Canvas under Lab 7. Place it in the same directory as this notebook and unzip it.\n",
    "\n",
    "We'll also need the <b>MNIST</b> data from the last lab (Lab 6, but also available under Lab 7). Download it from Canvas and put it under the same directory as this notebook and unzip it.\n",
    "\n",
    "First, load the text data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "iwords = np.loadtxt(\"words.imat.txt\")          # training data matrix in nnz x 3 form - rows are (doc, word, count) triples\n",
    "traincats = np.loadtxt(\"cats.imat.txt\")        # training labels in an ntrain x ncats matrix\n",
    "tiwords = np.loadtxt(\"testwords.imat.txt\")     # test data matrix in nnz x 3 form\n",
    "testcats = np.loadtxt(\"testcats.imat.txt\")     # test labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data come as dense matrices with (row, col, val) triples in their rows. But they represent sparse matrices so we do the conversion next. Note that the matrix constructor uses wordcount>0 tests instead of the actual word counts which has the effect of making the word features binary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = sp.csr_matrix((iwords[:,2].astype(\"int\") > 0, (iwords[:,1].astype(\"int\"), iwords[:,0].astype(\"int\"))))\n",
    "ntrain = train.shape[0]\n",
    "nfeats = train.shape[1]\n",
    "\n",
    "test = sp.csr_matrix((tiwords[:,2] > 0, (tiwords[:,1], tiwords[:,0])),shape=(4000,nfeats))  # need to match the number of cols (words)\n",
    "ntdocs = test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again we will concentrate on one label category, Category 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "traincat6 = traincats[:,6]\n",
    "testcat6 = testcats[:,6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll import a Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lrclassifier = LogisticRegression(max_iter = 120000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and train it on the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=120000000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrclassifier.fit(train,traincat6)\n",
    "\n",
    "#gave this error, so updated max_iter to 120000000\n",
    "#ConvergenceWarning: lbfgs failed to converge (status=1):\n",
    "# STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
    "# Increase the number of iterations (max_iter) or scale the data as shown in:\n",
    "#     https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "# Please also refer to the documentation for alternative solver options:\n",
    "#     https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
    "#   n_iter_i = _check_optimize_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = lrclassifier.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Question 1: Compute the accuracy of the predictions below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91025"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add code to score here (hint: this can be done in a single line of code)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(testcat6, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Question 2: How does this compare with Naive Bayes? In case you don't have the results handy, lets do it here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[BernoulliNB docs](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.756"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "# Add code to train, predict, score here (see sklearn documentation above for more info)\n",
    "\n",
    "#traditional order is - get target, which we do - traincat6\n",
    "#scale - standardScaler() - but we didn't do that the last regression either so we skip here for consistency\n",
    "#train_train_split\n",
    "#fit\n",
    "#accuracy \n",
    "\n",
    "#to avoid feeling like the annoucement could be at me \n",
    "#but also the length of these variables. RIP \n",
    "not_plagarised_fit = BernoulliNB().fit(train,traincat6)\n",
    "not_plagarised_nb_pred = not_plagarised_fit.predict(test)\n",
    "accuracy_score(testcat6, not_plagarised_nb_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When in doubt, logistic regression will carry the team through. I should probably add that to our final project for good measure but didn't want to because it was too \"simple\" but a simple algorithm will make you feel better about yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond Accuracy: ROC and AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label prediction accuracy is a useful but sometimes misleading measure. E.g., for data with 10% positives, a predictor that always says \"no\" will be 90% accurate. It is also very often useful to control the ratio of positive/negative labels to minimize a loss function. E.g., false positives are generally more acceptable in computational marketing (it means you show an ad to someone who might not be interested) than false negatives (you failed to show an ad to someone who might be interest, and might generate some revenue for you). \n",
    "\n",
    "Logistic Regression computes the probability of a label and that output is useful for both richer evaluation methods, and for making more careful tradeoffs between positives and negatives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC (Receiver-Operator Characteristic) curve is a very useful tool for interpreting classifier performance. See the background material here:\n",
    "https://en.wikipedia.org/wiki/Receiver_operating_characteristic .\n",
    "It shows the classifiers TPR (True-Positive Rate) vs FPR (False-Positive Rate) at various thresholds. The threshold isnt shown on the plot but can be inferred later. TPR and FPR are defined as:\n",
    "\n",
    "* TPR = TP / (TP + FN)   # based only on actual positive instances\n",
    "* FPR = FP / (FP + TN)   # based only on actual negative instances\n",
    "\n",
    "where TP = true positive, FN = false negative (actually a positive which is mislabelled), etc. \n",
    "Neither quantity involves a mix of positives and negatives. So ROC curves are insensitive to the actual ratio of positives to negatives. \n",
    "\n",
    "To use ROC, we first use a modified version of the \".predict\" method which returns label probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = lrclassifier.predict_proba(test);\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From which you can see that there are 2 columns, i.e., one probability of false and one for true. Verify that the sum of every row is 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(preds,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the probabilities of cat6 membership = true, which is column 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds6 = preds[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we'll do a ROC plot for it. ROC plots represent the performance of a classifier over a range of possible threshold values, showing the true positive rate and false positives rates at those thresholds. In Python, do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "rc = roc_curve(testcat6, preds6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns the X and Y coordinates of the ROC plot. To see it, do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2548b123dc0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(rc[0],rc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Question 3: What is the True positive rate at an FPR of 0.2 ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add your response here\n",
    "False Positive Rate is the X- Axis and the True Positive Rate is the Y- Axis. So an FPR of 0.2 will show a TPR of almost 1.  \n",
    "\n",
    "\n",
    "reference\n",
    "https://www.displayr.com/what-is-a-roc-curve-how-to-interpret-it/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the ROC curve is sometimes too much information, especially if you want to compare performance of many classifiers or datasets. The overall performance is well-characterized by the AUC or Area Under the Curve. Which is exactly what the name suggests, the area under the blue curve. Since a ROC plot lies in a 1 x 1 square, the area is always <= 1.0. A random predictor puts positives and negatives on a diagonal line with slope = 1, and so a random predictor has AUC = 0.5\n",
    "\n",
    "Lets check the AUC for our prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9637764912304416"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(testcat6, preds6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critical Thinking: Interpreting AUC scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUC score varies between 0.5 (random prediction) and 1.0. A common misconception is that a \"perfect\" predictor, i.e. a predictor that knows the exact probability of a label, will give a score of 1.0. That's incorrect. There are two sources of noise in the generation of a ROC plot:\n",
    "* The difference between the true and predicted probability of a label\n",
    "* The variance introduced by Bernoulli sampling to generate the label\n",
    "\n",
    "The latter is always present and depends on the distribution of label probabilities, the former depends on how good the model is. \n",
    "\n",
    "To see this, imagine a binary label distribution where each data label has a true probability of 0.5. A perfect predictor knows these probabilities but since they all the same, the sorted labels for the ROC plot would still be a random distribution of true and false. The ROC plot would have an AUC of 0.5. AUC scores very close to 1 are possible, but require that the true label distribution include a large fraction of probabilities close to either 1 or 0. That's because the variance of a Bernoulli variable is p(1-p), which is small if p is near 0 or 1. \n",
    "\n",
    "Let's estimate the ROC AUC for a perfect predictor on a similar distribution to our dataset. We can't know this distribution, but we can use the model's prediction  as an approximation to it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll generate some uniform random numbers in [0,1], one for each test point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = npr.random(testcat6.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll generate Bernoulli random numbers using the predictions as the underlying probability. We use the random numbers we just generated to do that. i.e. to generate a random Bernoulli variable with probability p, you generate a uniform random variable in [0,1] and test if (u < p). The probability that this test succeeds is exactly p. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (a < preds6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.987472882917213"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(x, preds6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this number with the AUC you computed earlier. To be clear again what this number is, it is the score of a *perfect* label predictor with the label probability distribution that our classifier has. It is an estimate of how well our classifier could do on this dataset. \n",
    "\n",
    "This secondary AUC calculation is a useful normalizing test when interpreting AUC scores. A common mistake is to assume that a model with AUC 0.85 on dataset A is better (i.e. would score higher on a common dataset) than a model with a score of 0.70 on dataset B. This is not true. It depends strongly on the dataset. The model with score 0.70 may be generating perfect or near-perfect predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.00e+00, 1.00e+00, 1.00e+00, 0.00e+00, 2.00e+00, 3.00e+00,\n",
       "        0.00e+00, 0.00e+00, 2.00e+00, 1.00e+00, 2.00e+00, 3.00e+00,\n",
       "        0.00e+00, 3.00e+00, 1.00e+00, 2.00e+00, 5.00e+00, 1.00e+01,\n",
       "        8.00e+00, 6.00e+00, 9.00e+00, 5.00e+00, 1.10e+01, 1.00e+01,\n",
       "        8.00e+00, 2.60e+01, 2.50e+01, 2.00e+01, 2.60e+01, 3.20e+01,\n",
       "        3.30e+01, 4.80e+01, 4.10e+01, 6.00e+01, 6.60e+01, 7.00e+01,\n",
       "        6.90e+01, 9.60e+01, 1.08e+02, 1.19e+02, 1.04e+02, 1.27e+02,\n",
       "        1.38e+02, 1.42e+02, 1.22e+02, 1.31e+02, 1.14e+02, 1.27e+02,\n",
       "        1.52e+02, 1.91e+03]),\n",
       " array([-1.11982006e+01, -1.09742366e+01, -1.07502726e+01, -1.05263086e+01,\n",
       "        -1.03023446e+01, -1.00783806e+01, -9.85441654e+00, -9.63045253e+00,\n",
       "        -9.40648852e+00, -9.18252451e+00, -8.95856049e+00, -8.73459648e+00,\n",
       "        -8.51063247e+00, -8.28666846e+00, -8.06270444e+00, -7.83874043e+00,\n",
       "        -7.61477642e+00, -7.39081241e+00, -7.16684839e+00, -6.94288438e+00,\n",
       "        -6.71892037e+00, -6.49495636e+00, -6.27099235e+00, -6.04702833e+00,\n",
       "        -5.82306432e+00, -5.59910031e+00, -5.37513630e+00, -5.15117228e+00,\n",
       "        -4.92720827e+00, -4.70324426e+00, -4.47928025e+00, -4.25531623e+00,\n",
       "        -4.03135222e+00, -3.80738821e+00, -3.58342420e+00, -3.35946018e+00,\n",
       "        -3.13549617e+00, -2.91153216e+00, -2.68756815e+00, -2.46360414e+00,\n",
       "        -2.23964012e+00, -2.01567611e+00, -1.79171210e+00, -1.56774809e+00,\n",
       "        -1.34378407e+00, -1.11982006e+00, -8.95856049e-01, -6.71892037e-01,\n",
       "        -4.47928025e-01, -2.23964012e-01, -1.57446228e-10]),\n",
       " <BarContainer object of 50 artists>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(np.log10(preds6),50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is an enormous range of values. Most probabilities are very close to zero or one, which is why this dataset has such a high ROC AUC score.\n",
    "\n",
    "Suppose instead we had a dataset with a less wide distribution. We can use a lognormal distribution to simulate this for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cprob = np.minimum(npr.lognormal(-4,1,10000),1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a pretty good model, e.g. for the range of user's probabilities of clicking on an ad. Let's look at a histogram of the log10 of the values (a direct histogram will be too squashed near 1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   8.,   15.,   35.,   78.,  175.,  350.,  558.,  865., 1111.,\n",
       "        1356., 1378., 1314., 1064.,  726.,  464.,  252.,  149.,   70.,\n",
       "          24.,    8.]),\n",
       " array([-3.306476  , -3.15389576, -3.00131552, -2.84873527, -2.69615503,\n",
       "        -2.54357479, -2.39099455, -2.2384143 , -2.08583406, -1.93325382,\n",
       "        -1.78067358, -1.62809334, -1.47551309, -1.32293285, -1.17035261,\n",
       "        -1.01777237, -0.86519212, -0.71261188, -0.56003164, -0.4074514 ,\n",
       "        -0.25487116]),\n",
       " <BarContainer object of 20 artists>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(np.log10(cprob),20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that's our distribution of virtual users. Notice that the values (which represent click probabilities) range over several orders of magnitude since we plotted their log10. Next we simulate users' click behavior. Once again we generate a uniform random variable u for each user, and output 1 if u < the user's click probability given by cprob. \n",
    "\n",
    "Finally we compute the AUC on that data, which is the score of a perfect predictor on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7650910033437555"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = npr.random(cprob.shape)\n",
    "x = (a < cprob)\n",
    "roc_auc_score(x, cprob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that's the AUC score for a perfect predictor on this (artificial) dataset. This is lower than the *real* predictions on the RCV1 text dataset. So be careful when interpreting AUC scores. There is no absolute scale for them, and they depend a lot on the dataset.\n",
    "\n",
    "Another important point is that the AUC value for mid-range scores can have quite a lot of variance. Try re-evaluating the last cell to see what happens. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Question 4: What changes do you think you should make to the distribution cprob to increase the ROC AUC score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add your response here  \n",
    "\n",
    "Scaling would probably be helpful. I didn't see a scaling function being used. This normalizes the data and makes sure that we even out a lot of the outliers. With only some thousand datapoints, we also don't notice a big change in performance but when the data scales, so will the time if we don't scale it, but the last point says that there is no scaling? \n",
    "\n",
    "Based on the ROC-AUC curve too, it looks like it's a binary classification, so reducing the number of classes/features would be another way to increase the the score but we only have two features. So, if no scaling and no multiclass classification then to increase the score,   \n",
    "\n",
    "...we use actual data and not random  \n",
    "\n",
    "but to be fair, this isn't bad for random numbers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forests are an extremely accurate classifier for datasets of moderate size. Let's try them out here. We'll load the MNIST data now, but first its probably a good idea to restart your kernel to reduce memory use. Click on the \"Kernel\" menu above and then \"Restart\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train0=np.loadtxt(\"train.fmat.txt\")\n",
    "test0=np.loadtxt(\"test.fmat.txt\")\n",
    "train = np.transpose(train0[:,0:4000])\n",
    "test = np.transpose(test0[:,0:2000])\n",
    "traincats = np.loadtxt(\"ictrain.imat.txt\")\n",
    "testcats = np.loadtxt(\"ictest.imat.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we're going to tuning the parameters of RFs on some test data, we need to split our test set into a validation set and a final test set to avoid overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = test[0:1000,:]\n",
    "finaltest = test[1000:2000,:]\n",
    "validationcats = testcats[0:1000]\n",
    "finaltestcats = testcats[1000:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, criterion='entropy', max_features=30,\n",
       "                       n_jobs=4)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfclassifier = RandomForestClassifier(criterion='entropy',max_features=30,n_estimators=100,n_jobs=4,bootstrap=False)\n",
    "rfclassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.925"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfclassifier.fit(train,traincats)\n",
    "preds = rfclassifier.predict(validation)\n",
    "np.mean(preds == validationcats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the [scikit-learn documentation for Random Forests](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html?highlight=random%20forest#sklearn.ensemble.RandomForestClassifier) to make sure you understand the meaning of all the parameters in the call to the RandForestClassifier constructor. Which ones do you think will improve accuracy the most? **NOTE** you don't need to tune n_jobs. Its the number of threads that the classifier code runs and it only affects running time. It should be set to the number of cores that your processor has. \n",
    "\n",
    "Try tuning the classifier with the validation set above to get better than 90% accuracy on the validation set. Don't touch the final test set until you're done tuning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Question 5: Make a table with at least two values you tried each for criterion, max_features, n_estimators, and bootstrap. What trends to you notice for each one? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Criteria</th>\n",
       "      <th>max_features</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>n_jobs</th>\n",
       "      <th>bootstrap</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gini</td>\n",
       "      <td>30</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>0.897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>entropy</td>\n",
       "      <td>30</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>0.897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gini</td>\n",
       "      <td>50</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>0.880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>entropy</td>\n",
       "      <td>50</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>0.893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entropy</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>0.873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>0.880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gini</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>0.852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>entropy</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>0.859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gini</td>\n",
       "      <td>30</td>\n",
       "      <td>100</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>0.924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>entropy</td>\n",
       "      <td>30</td>\n",
       "      <td>100</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>0.925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>gini</td>\n",
       "      <td>30</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>0.913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>entropy</td>\n",
       "      <td>30</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>0.921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>gini</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>0.627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>entropy</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>0.633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>gini</td>\n",
       "      <td>30</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>entropy</td>\n",
       "      <td>30</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>gini</td>\n",
       "      <td>30</td>\n",
       "      <td>100</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>entropy</td>\n",
       "      <td>30</td>\n",
       "      <td>100</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>gini</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>entropy</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>gini</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>entropy</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>gini</td>\n",
       "      <td>100</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>entropy</td>\n",
       "      <td>100</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>gini</td>\n",
       "      <td>30</td>\n",
       "      <td>100</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>entropy</td>\n",
       "      <td>30</td>\n",
       "      <td>100</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>gini</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>entropy</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.930</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Criteria  max_features  n_estimators  n_jobs  bootstrap  accuracy\n",
       "0      gini            30            20       4       True     0.897\n",
       "1   entropy            30            20       4       True     0.897\n",
       "2      gini            50            20       4       True     0.880\n",
       "3   entropy            50            20       4       True     0.893\n",
       "4   entropy            10            20       4       True     0.873\n",
       "5      gini            10            20       4       True     0.880\n",
       "6      gini             1            20       4       True     0.852\n",
       "7   entropy             1            20       4       True     0.859\n",
       "8      gini            30           100       4       True     0.924\n",
       "9   entropy            30           100       4       True     0.925\n",
       "10     gini            30            50       4       True     0.913\n",
       "11  entropy            30            50       4       True     0.921\n",
       "12     gini            30             1       4       True     0.627\n",
       "13  entropy            30             1       4       True     0.633\n",
       "14     gini            30            20       4      False     0.904\n",
       "15  entropy            30            20       4      False     0.908\n",
       "16     gini            30           100       4      False     0.930\n",
       "17  entropy            30           100       4      False     0.932\n",
       "18     gini             1            20       4      False     0.858\n",
       "19  entropy             1            20       4      False     0.870\n",
       "20     gini            30             1       4      False     0.678\n",
       "21  entropy            30             1       4      False     0.671\n",
       "22     gini           100            20       4      False     0.906\n",
       "23  entropy           100            20       4      False     0.903\n",
       "24     gini            30           100       4      False     0.929\n",
       "25  entropy            30           100       4      False     0.924\n",
       "26     gini           100           100       4      False     0.919\n",
       "27  entropy           100           100       4      False     0.930"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add your code and response for Question 5 here\n",
    "\n",
    "#criterion = \"gini\" and \"entropy\"\n",
    "#max_features for best split\n",
    "#n_estimators = # of trees in forest\n",
    "#bootstrap = bootstrap samples is a resampling technique \n",
    "\n",
    "#oohhh man, this might be easier with an excel sheet and manually typing it in if we're going to be observing it \n",
    "#I will manually type this in if needed, but for my lazy self, I'd really hope I don't have to. \n",
    "not_plagarised_df = pd.read_excel(\"DS Lab7 - Random Forest.xlsx\")\n",
    "not_plagarised_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more trees there are(n_estimators), the higher the accuracy and vice versa with less trees  \n",
    "Surprisingly, when I went down in the number of features, it didn't drastically change as much as I would think.   \n",
    "Neither was changing the Bootstrap resampling. If the accuracy was low with bootstrapping, then it will be low without it.\n",
    "I could test the limits some more and go even higher on number of trees or features, but I was mostly testing which one would show the biggest change if I changed it to 1 and tank the accuracy. Higher number of features and higher number of trees will improve accuracy.  This is the new  thing that I have tried where I increased one or the other to 1000 and then the last tune is when I set them both to 1000.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Question 6: Report your validation and final test accuracy. Include all the parameters you used, e.g., include the line where you invoked the RandomForestClassifier constructor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be using the max value in accuracy out of my tests and put the values back into the randomForestClassifier constructor given to us when I was playing around with the variables. We are calling the constructor with the settings on the predict function.  \n",
    "That is my final test accuracy score, a 0.895. RIP\n",
    "I need to go back and change the values some more and see if I can get it higher. Will update and see what happens when I yolo the number of trees and number of features   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Criteria        entropy\n",
       "max_features         30\n",
       "n_estimators        100\n",
       "n_jobs                4\n",
       "bootstrap         False\n",
       "accuracy          0.932\n",
       "Name: 17, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add your response/code for Question 6 here (either a written response or reporting with code are fine)\n",
    "max_accuracy = not_plagarised_df.iloc[not_plagarised_df['accuracy'].idxmax()]\n",
    "max_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.893"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = rfclassifier.predict(finaltest)\n",
    "np.mean(preds == finaltestcats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.925"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = rfclassifier.predict(validation)\n",
    "np.mean(preds == validationcats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Question 7: Reflect on and explain any differences between your validation and final test accuracy scores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add your response here\n",
    "Lol my final test accuracy score is lower than the validation prediction score and I'm a little mad about it. It means that the settings that I put in the algorithm isn't as good to match up with the actual data so I need to go back and manipulate it a little more\n",
    "So after playing around with it. There is a sweet spot where the data will perform at the best that it can perform. Overshooting the variables do not do much for it and unfortunately, I am still mad that the random forest data changes with every restart so this messes with my excel data which is hardcoded in. A way to probably increase the accuracy at this rate would be to just throw it through an automator, where it would brute force all of the combinations of parameters and record them and then find the max value after hitting the top where the forest burns and then testing the prediction that way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference between my tuning vs validation set is 0.10486486486486486\n"
     ]
    }
   ],
   "source": [
    "print(\"Difference between my tuning vs validation set is\", np.mean(preds == finaltestcats)/np.mean(preds == validationcats) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
